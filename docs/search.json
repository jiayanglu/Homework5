[
  {
    "objectID": "Homework_5.html",
    "href": "Homework_5.html",
    "title": "Homework5_ST558_Jia Lu",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\n\n\nThe purpose of using cross-validation when fitting a random forest model is to help in evaluating performance more accurately, to select optimal tuning parameter, to protect from overfitting, and to get more robust performance metrics.\n\n\nDescribe the bagged tree algorithm.\n\n\nThe bagged tree algorithm utilizes the bagging (bootstrap aggregation) method, which involves repeatedly drawing bootstrap samples from either the dataset (non-parametric approach) or a fitted model (parametric approach). For each bootstrap sample, a decision tree is trained independently. After training, predictions from individual trees are aggregated. This ensemble approach reduces variance and enhances model robustness, leading to improved predictive performance compared to a single decision tree.\n\n\nWhat is meant by a general linear model?\n\n\nGeneral linear model uses continuous response variables that meet normality assumptions and allows for both continuous and categorical predictors. The relationship between predictors and the response is linear.\n\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\n\nFitting a multiple linear regression model that includes an interaction term allows the model to capture non-additive relationships among variables. This accommodates more complexity in the relationship between predictors and the response variable, potentially leading to more accurate predictions compared to a model without interaction terms.\n\n\nWhy do we split our data into a training and test set?\n\n\nSplitting data into training and test sets ensures that machine learning models are evaluated on unseen data, preventing overfitting and providing a realistic measure of their predictive performance. It also allows for effective hyperparameter tuning and ensures the model’s reliability for real-world applications."
  },
  {
    "objectID": "Homework_5.html#quick-edadata-preparation",
    "href": "Homework_5.html#quick-edadata-preparation",
    "title": "Homework5_ST558_Jia Lu",
    "section": "Quick EDA/Data Preparation",
    "text": "Quick EDA/Data Preparation\n\nQuickly understand data: Check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease.\n\n\n#read in csv data\ndata &lt;- read_csv('heart.csv')\n\n#check total NA data entries \ncolSums(is.na(data))\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n\nFrom the result, there is no NA entries in this data set.\n\n#check which column has zero values in it and how many of them\ncolSums(data == 0)\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              1            172 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n           704              0              0              0            368 \n      ST_Slope   HeartDisease \n             0            410 \n\n\nFrom the result, we can see there are one zero entry (missing measurement) from RestingBP, and 172 zero entries from Cholesterol. These variables are all numeric variables which might have relationships to HeartDisease. Although FastingBS and Oldpeak also have many zero entries, the value of zero is plausible for these two variables, not due to missing data.\nDrop patients with zero value entries for variable RestingBP and variable Cholesterol from this data set.\n\ndata &lt;- data |&gt;\n  filter(!RestingBP == 0 & !Cholesterol == 0)\ndata\n\n# A tibble: 746 × 12\n     Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1    40 M     ATA                 140         289         0 Normal       172\n 2    49 F     NAP                 160         180         0 Normal       156\n 3    37 M     ATA                 130         283         0 ST            98\n 4    48 F     ASY                 138         214         0 Normal       108\n 5    54 M     NAP                 150         195         0 Normal       122\n 6    39 M     NAP                 120         339         0 Normal       170\n 7    45 F     ATA                 130         237         0 Normal       170\n 8    54 M     ATA                 110         208         0 Normal       142\n 9    37 M     ASY                 140         207         0 Normal       130\n10    48 F     ATA                 120         284         0 Normal       120\n# ℹ 736 more rows\n# ℹ 4 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, ST_Slope &lt;chr&gt;,\n#   HeartDisease &lt;dbl&gt;\n\n\nMore EDA is conducted on this data set. I create new dummy variables for HeartDisease, and calculate and plot correlation between HeartDisease_character1 and other variables.\n\n#change variable HeartDisease to be a character variable for creating dummy variable purpose\ndata$HeartDisease_character &lt;- as.character(data$HeartDisease)\nstr(data)\n\nspc_tbl_ [746 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Age                   : num [1:746] 40 49 37 48 54 39 45 54 37 48 ...\n $ Sex                   : chr [1:746] \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType         : chr [1:746] \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP             : num [1:746] 140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol           : num [1:746] 289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS             : num [1:746] 0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG            : chr [1:746] \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR                 : num [1:746] 172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina        : chr [1:746] \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak               : num [1:746] 0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ ST_Slope              : chr [1:746] \"Up\" \"Flat\" \"Up\" \"Flat\" ...\n $ HeartDisease          : num [1:746] 0 1 0 1 0 0 0 0 1 0 ...\n $ HeartDisease_character: chr [1:746] \"0\" \"1\" \"0\" \"1\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Age = col_double(),\n  ..   Sex = col_character(),\n  ..   ChestPainType = col_character(),\n  ..   RestingBP = col_double(),\n  ..   Cholesterol = col_double(),\n  ..   FastingBS = col_double(),\n  ..   RestingECG = col_character(),\n  ..   MaxHR = col_double(),\n  ..   ExerciseAngina = col_character(),\n  ..   Oldpeak = col_double(),\n  ..   ST_Slope = col_character(),\n  ..   HeartDisease = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n#create new dummy variables via dummyVars() and predict() \ncategorical_vars &lt;- c(\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"HeartDisease_character\")\ndummy_model &lt;- dummyVars(~ ., data = data[, categorical_vars])\ndata_dummies &lt;- predict(dummy_model, newdata = data)\n#add these columns to our data frame\ndata_2 &lt;- cbind(data_dummies, data[, !names(data) %in% categorical_vars])\nstr(data_2)\n\n'data.frame':   746 obs. of  21 variables:\n $ SexF                   : num  0 1 0 1 0 0 1 0 0 1 ...\n $ SexM                   : num  1 0 1 0 1 1 0 1 1 0 ...\n $ ChestPainTypeASY       : num  0 0 0 1 0 0 0 0 1 0 ...\n $ ChestPainTypeATA       : num  1 0 1 0 0 0 1 1 0 1 ...\n $ ChestPainTypeNAP       : num  0 1 0 0 1 1 0 0 0 0 ...\n $ ChestPainTypeTA        : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGLVH          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGNormal       : num  1 1 0 1 1 1 1 1 1 1 ...\n $ RestingECGST           : num  0 0 1 0 0 0 0 0 0 0 ...\n $ ExerciseAnginaN        : num  1 1 1 0 1 1 1 1 0 1 ...\n $ ExerciseAnginaY        : num  0 0 0 1 0 0 0 0 1 0 ...\n $ HeartDisease_character0: num  1 0 1 0 1 1 1 1 0 1 ...\n $ HeartDisease_character1: num  0 1 0 1 0 0 0 0 1 0 ...\n $ Age                    : num  40 49 37 48 54 39 45 54 37 48 ...\n $ RestingBP              : num  140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol            : num  289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS              : num  0 0 0 0 0 0 0 0 0 0 ...\n $ MaxHR                  : num  172 156 98 108 122 170 170 142 130 120 ...\n $ Oldpeak                : num  0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ ST_Slope               : chr  \"Up\" \"Flat\" \"Up\" \"Flat\" ...\n $ HeartDisease           : num  0 1 0 1 0 0 0 0 1 0 ...\n\nm &lt;- cor(select(data_2, -c(\"HeartDisease_character0\", \"SexF\", \"RestingECGNormal\", \"ExerciseAnginaN\", \"HeartDisease\", \"ST_Slope\")))\n\ncorrplot(m, method = 'color',\n         order = 'alphabet',\n         diag = FALSE,\n         col = COL2('RdBu'),\n         tl.cex = 0.7)\n\n\n\n\n\n\n\n\nFrom the graph above it, we can see that there are some positive linear correlation between HeartDisease_character1 and ExerciseAnginaY, ChestPainTypeASY, Oldpeak, Age, or SexM, and some negative linear correlation between HeartDisease1 and MaxHR, ChestPainTypeATA, or ChestPainTypeNAP. There are also some weak relationships between HeartDisease_character1 and RestingBP or FastingBS. It seems that Cholesterol, RestingECGLVH, and RestingECGST don’t have strong correlations with HeartDisease_character1. Additionally, there is also some negative correlation between Age and MaxHR, and some positive correlation between ExerciseAnginaY and ChestPainTypeASY.\nSimilar result is obtained using GGally::ggpairs().\n\nGGally::ggpairs(data)\n\n\n\n\n\n\n\n\n\nCreate a new variable that is a factor version of the HeartDisease variable. Remove the ST_Slope variable, the original HeartDisease variable, and the HeartDisease_character variable.\n\n\ndata &lt;- data |&gt;\n  mutate(HeartDisease_factor = as.factor(HeartDisease)) |&gt;\n  select(everything(), -ST_Slope, -HeartDisease_character, -HeartDisease)\nstr(data)\n\ntibble [746 × 11] (S3: tbl_df/tbl/data.frame)\n $ Age                : num [1:746] 40 49 37 48 54 39 45 54 37 48 ...\n $ Sex                : chr [1:746] \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType      : chr [1:746] \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP          : num [1:746] 140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol        : num [1:746] 289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS          : num [1:746] 0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG         : chr [1:746] \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR              : num [1:746] 172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina     : chr [1:746] \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak            : num [1:746] 0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDisease_factor: Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ...\n\n\n\nCreate dummy columns corresponding to the values of these three variables for use in our kNN fit. The caret vignette has a function to help us out here. You should use dummyVars() and predict() to create new columns. Then add these columns to our data frame.\n\nFrom the output after using str(), we have some categorical predictors still in our data set: Sex, ChestPainType, RestingECG, and ExerciseAngina.\n\n#specify all categorical variables left in data\ncategorical_vars_2 &lt;- c(\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\")\n#create new columns for dummy variables via dummyVars() and predict() \ndummy_model_2 &lt;- dummyVars(~ ., data = data[, categorical_vars_2])\ndata_dummies_2 &lt;- predict(dummy_model_2, newdata = data)\n#add these columns to our data frame\ndata_knn &lt;- cbind(data_dummies_2, data[, !names(data) %in% categorical_vars_2])\n\nstr(data_knn)\n\n'data.frame':   746 obs. of  18 variables:\n $ SexF               : num  0 1 0 1 0 0 1 0 0 1 ...\n $ SexM               : num  1 0 1 0 1 1 0 1 1 0 ...\n $ ChestPainTypeASY   : num  0 0 0 1 0 0 0 0 1 0 ...\n $ ChestPainTypeATA   : num  1 0 1 0 0 0 1 1 0 1 ...\n $ ChestPainTypeNAP   : num  0 1 0 0 1 1 0 0 0 0 ...\n $ ChestPainTypeTA    : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGLVH      : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGNormal   : num  1 1 0 1 1 1 1 1 1 1 ...\n $ RestingECGST       : num  0 0 1 0 0 0 0 0 0 0 ...\n $ ExerciseAnginaN    : num  1 1 1 0 1 1 1 1 0 1 ...\n $ ExerciseAnginaY    : num  0 0 0 1 0 0 0 0 1 0 ...\n $ Age                : num  40 49 37 48 54 39 45 54 37 48 ...\n $ RestingBP          : num  140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol        : num  289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS          : num  0 0 0 0 0 0 0 0 0 0 ...\n $ MaxHR              : num  172 156 98 108 122 170 170 142 130 120 ...\n $ Oldpeak            : num  0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDisease_factor: Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ..."
  },
  {
    "objectID": "Homework_5.html#split-your-data",
    "href": "Homework_5.html#split-your-data",
    "title": "Homework5_ST558_Jia Lu",
    "section": "Split your Data",
    "text": "Split your Data\nSplit this data into a training and test set.\n\nset.seed(11)\n\ntrainIndex &lt;- createDataPartition(data_knn$HeartDisease_factor, p = .7,\n                                  list = FALSE,\n                                  times = 1)\n\ntrain_data &lt;-  data_knn[trainIndex, ]\ntest_data &lt;- data_knn[-trainIndex, ]\n\n#check the dimensions of our training data and testing data frame\ndim(train_data)\n\n[1] 523  18\n\ndim(test_data)\n\n[1] 223  18"
  },
  {
    "objectID": "Homework_5.html#knn",
    "href": "Homework_5.html#knn",
    "title": "Homework5_ST558_Jia Lu",
    "section": "kNN",
    "text": "kNN\nCheck summarized details of our data using summary() method. It will give us a basic idea about our dataset’s attributes range.\n\nsummary(data_knn)\n\n      SexF            SexM       ChestPainTypeASY ChestPainTypeATA\n Min.   :0.000   Min.   :0.000   Min.   :0.000    Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:1.000   1st Qu.:0.000    1st Qu.:0.0000  \n Median :0.000   Median :1.000   Median :0.000    Median :0.0000  \n Mean   :0.244   Mean   :0.756   Mean   :0.496    Mean   :0.2225  \n 3rd Qu.:0.000   3rd Qu.:1.000   3rd Qu.:1.000    3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.000   Max.   :1.000    Max.   :1.0000  \n ChestPainTypeNAP ChestPainTypeTA   RestingECGLVH    RestingECGNormal\n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.00000   Median :0.0000   Median :1.0000  \n Mean   :0.2265   Mean   :0.05496   Mean   :0.2359   Mean   :0.5965  \n 3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  RestingECGST    ExerciseAnginaN  ExerciseAnginaY       Age       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :28.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:46.00  \n Median :0.0000   Median :1.0000   Median :0.0000   Median :54.00  \n Mean   :0.1676   Mean   :0.6153   Mean   :0.3847   Mean   :52.88  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:59.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :77.00  \n   RestingBP    Cholesterol      FastingBS          MaxHR      \n Min.   : 92   Min.   : 85.0   Min.   :0.0000   Min.   : 69.0  \n 1st Qu.:120   1st Qu.:207.2   1st Qu.:0.0000   1st Qu.:122.0  \n Median :130   Median :237.0   Median :0.0000   Median :140.0  \n Mean   :133   Mean   :244.6   Mean   :0.1676   Mean   :140.2  \n 3rd Qu.:140   3rd Qu.:275.0   3rd Qu.:0.0000   3rd Qu.:160.0  \n Max.   :200   Max.   :603.0   Max.   :1.0000   Max.   :202.0  \n    Oldpeak        HeartDisease_factor\n Min.   :-0.1000   0:390              \n 1st Qu.: 0.0000   1:356              \n Median : 0.5000                      \n Mean   : 0.9016                      \n 3rd Qu.: 1.5000                      \n Max.   : 6.2000                      \n\n\nBefore modeling, let’s scale and centralized data.\n\npre_proc_values &lt;- preProcess(train_data, method = c(\"center\", \"scale\"))\n\n#Scaling and centralizing train and test data sets.\ntrain_transformed &lt;- predict(pre_proc_values, train_data)\ntest_transformed &lt;- predict(pre_proc_values, test_data)\n\nFrom above summary statistics, it shows us that all the attributes have a different range. So, we need to standardize our data. We can standardize data using caret’s preProcess() method.\nCaret package provides train() method for training our data for various algorithms. Before train() method, we will first use trainControl() method. It controls the computational nuances of the train() method.\nFor training knn classifier, train() method should be passed with “method” parameter as “knn”. The “trControl” parameter should be passed with results from our trianControl() method. The “tuneLength” parameter holds an integer value. This is for tuning our algorithm.\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\nset.seed(11)\nknn_fit &lt;- train(HeartDisease_factor ~ ., \n                 data = train_transformed, \n                 method = \"knn\",\n                 trControl=trctrl,\n                 tuneGrid = expand.grid(k = 1:40),\n                 tuneLength = 10)\nknn_fit\n\nk-Nearest Neighbors \n\n523 samples\n 17 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 471, 470, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7540881  0.5065335\n   2  0.7680576  0.5346613\n   3  0.8031447  0.6054836\n   4  0.8031930  0.6058569\n   5  0.8177310  0.6350432\n   6  0.8017658  0.6035295\n   7  0.8158684  0.6315778\n   8  0.8151790  0.6301061\n   9  0.8164731  0.6328184\n  10  0.8138728  0.6276469\n  11  0.8106555  0.6212254\n  12  0.8144533  0.6285959\n  13  0.8183116  0.6362887\n  14  0.8202346  0.6399818\n  15  0.8246009  0.6486054\n  16  0.8272134  0.6537991\n  17  0.8265239  0.6522635\n  18  0.8220247  0.6434136\n  19  0.8175738  0.6345268\n  20  0.8194606  0.6384676\n  21  0.8124456  0.6244117\n  22  0.8194848  0.6383785\n  23  0.8156628  0.6307555\n  24  0.8163159  0.6318452\n  25  0.8201500  0.6395056\n  26  0.8214320  0.6421701\n  27  0.8214441  0.6421971\n  28  0.8252661  0.6498578\n  29  0.8246493  0.6485290\n  30  0.8182753  0.6357533\n  31  0.8215046  0.6422774\n  32  0.8221214  0.6435894\n  33  0.8201984  0.6397962\n  34  0.8215046  0.6423420\n  35  0.8189163  0.6370657\n  36  0.8214925  0.6421690\n  37  0.8189405  0.6370630\n  38  0.8150822  0.6292948\n  39  0.8182874  0.6358193\n  40  0.8138123  0.6269484\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 16.\n\n\nThe output shows Accuracy and Kappa metrics for different k value and it automatically selects best k-value (k=16).\nNow, our model is trained with K value as 16 We are ready to predict HeartDisease for our test set. We can use predict() method.\n\ntest_pred &lt;- predict(knn_fit, newdata = select(test_transformed, -HeartDisease_factor))\n\nTo check how well this chosen model does on the test set, we can use the confusionMatrix() function.\n\nconfusion_matrix_knn &lt;- confusionMatrix(test_pred, test_transformed$HeartDisease_factor)\nconfusion_matrix_knn\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 91 27\n         1 26 79\n                                          \n               Accuracy : 0.7623          \n                 95% CI : (0.7009, 0.8166)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 2.218e-13       \n                                          \n                  Kappa : 0.5233          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.7778          \n            Specificity : 0.7453          \n         Pos Pred Value : 0.7712          \n         Neg Pred Value : 0.7524          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4081          \n   Detection Prevalence : 0.5291          \n      Balanced Accuracy : 0.7615          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nIt shows that our model accuracy for test set is 76.23%."
  },
  {
    "objectID": "Homework_5.html#logistic-regression",
    "href": "Homework_5.html#logistic-regression",
    "title": "Homework5_ST558_Jia Lu",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nLogistic regression Model 1\nTo create logistic regression models, we don’t need to create the dummy variables. Thus, we can use the previous data set “data” in the training process.\nFitting a model with ExerciseAngina, ChestPainType, Oldpeak, Age, MaxHR and Sex as predictors.\n\nstr(data)\n\ntibble [746 × 11] (S3: tbl_df/tbl/data.frame)\n $ Age                : num [1:746] 40 49 37 48 54 39 45 54 37 48 ...\n $ Sex                : chr [1:746] \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType      : chr [1:746] \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP          : num [1:746] 140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol        : num [1:746] 289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS          : num [1:746] 0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG         : chr [1:746] \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR              : num [1:746] 172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina     : chr [1:746] \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak            : num [1:746] 0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDisease_factor: Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ...\n\nset.seed(11)\n\n#split this data into a training and test set\ntrainIndex_2 &lt;- createDataPartition(data$HeartDisease_factor, p = .7,\n                                    list = FALSE,\n                                    times = 1)\n\ntrain_data_2 &lt;-  data[trainIndex_2, ]\ntest_data_2 &lt;- data[-trainIndex_2, ]\n#check the dimensions of our training data and testing data frame\ndim(train_data_2)\n\n[1] 523  11\n\ndim(test_data_2)\n\n[1] 223  11\n\n#preprocessing data\npre_proc_values_2 &lt;- preProcess(train_data_2, method = c(\"center\", \"scale\"))\n\n#Scaling and centralizing train and test data sets.\ntrain_transformed_2 &lt;- predict(pre_proc_values_2, train_data_2)\ntest_transformed_2 &lt;- predict(pre_proc_values_2, test_data_2)\n\nlogistic_M1_fit &lt;- train(HeartDisease_factor ~ ExerciseAngina + ChestPainType + Oldpeak + Age + MaxHR + Sex, \n                         data = train_transformed_2, \n                         method = \"glm\",\n                         family=\"binomial\",\n                         trControl=trctrl)\nlogistic_M1_fit\n\nGeneralized Linear Model \n\n523 samples\n  6 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 470, 471, 471, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8292574  0.6571128\n\n\n\n\nLogistic regression Model 2\nFitting a model with ExerciseAngina, ChestPainType, Oldpeak, Age, MaxHR and Sex, and the interaction term between Age and MaxHR as predictors.\n\nset.seed(11)\nlogistic_M2_fit &lt;- train(HeartDisease_factor ~ ExerciseAngina + ChestPainType + Oldpeak + Age + MaxHR + Sex + Age:MaxHR, \n                         data = train_transformed_2, \n                         method = \"glm\",\n                         family=\"binomial\",\n                         trControl=trctrl,\n                         preProcess = c(\"center\", \"scale\"))\nlogistic_M2_fit\n\nGeneralized Linear Model \n\n523 samples\n  6 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (9), scaled (9) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 471, 470, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8335269  0.6659332\n\n\n\n\nLogistic regression Model 3\nFitting a model with ExerciseAngina, ChestPainType, Oldpeak, Age, MaxHR and Sex, the interaction term between Age and MaxHR, and between ExerciseAngina and ChestPainType as predictors.\n\nset.seed(11)\nlogistic_M3_fit &lt;- train(HeartDisease_factor ~ ExerciseAngina + ChestPainType + Oldpeak + Age + MaxHR + Sex + Age:MaxHR + ExerciseAngina:ChestPainType, \n                         data = train_transformed_2, \n                         method = \"glm\",\n                         family=\"binomial\",\n                         trControl=trctrl,\n                         preProcess = c(\"center\", \"scale\"))\nlogistic_M3_fit\n\nGeneralized Linear Model \n\n523 samples\n  6 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (12), scaled (12) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 471, 470, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8258345  0.6507503\n\n\n\n\nModels comparison\nObtain basic summary and train accuracy for each model:\n\n#summary and accuracy for logistic M1 model:\nsummary(logistic_M1_fit)\n\n\nCall:\nNULL\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.0313     0.3167  -3.257  0.00113 ** \nExerciseAnginaY    1.3451     0.2794   4.814 1.48e-06 ***\nChestPainTypeATA  -1.8349     0.3726  -4.924 8.48e-07 ***\nChestPainTypeNAP  -1.5456     0.3212  -4.812 1.49e-06 ***\nChestPainTypeTA   -0.8061     0.5093  -1.583  0.11350    \nOldpeak            0.8611     0.1563   5.508 3.62e-08 ***\nAge                0.2598     0.1434   1.811  0.07008 .  \nMaxHR             -0.3791     0.1426  -2.658  0.00787 ** \nSexM               1.5854     0.3209   4.940 7.81e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 724.02  on 522  degrees of freedom\nResidual deviance: 396.99  on 514  degrees of freedom\nAIC: 414.99\n\nNumber of Fisher Scoring iterations: 5\n\nlog_reg_model_1_train_acc &lt;- logistic_M1_fit$results$Accuracy\nlog_reg_model_1_train_acc\n\n[1] 0.8292574\n\nfitted_M1 &lt;- predict(logistic_M1_fit,\n                     newdata = select(test_transformed_2, -HeartDisease_factor), type='raw')\n\nconfusion_matrix_M1 &lt;- confusionMatrix(fitted_M1, test_transformed_2$HeartDisease_factor)\nconfusion_matrix_M1\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 97 28\n         1 20 78\n                                          \n               Accuracy : 0.7848          \n                 95% CI : (0.7249, 0.8368)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 7.614e-16       \n                                          \n                  Kappa : 0.5669          \n                                          \n Mcnemar's Test P-Value : 0.3123          \n                                          \n            Sensitivity : 0.8291          \n            Specificity : 0.7358          \n         Pos Pred Value : 0.7760          \n         Neg Pred Value : 0.7959          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4350          \n   Detection Prevalence : 0.5605          \n      Balanced Accuracy : 0.7825          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#summary and accuracy for logistic M2 model:\nsummary(logistic_M2_fit)\n\n\nCall:\nNULL\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -0.115257   0.133244  -0.865  0.38703    \nExerciseAnginaY   0.659364   0.137000   4.813 1.49e-06 ***\nChestPainTypeATA -0.767503   0.155959  -4.921 8.60e-07 ***\nChestPainTypeNAP -0.644370   0.134173  -4.803 1.57e-06 ***\nChestPainTypeTA  -0.181222   0.115074  -1.575  0.11529    \nOldpeak           0.861225   0.156377   5.507 3.64e-08 ***\nAge               0.259045   0.144234   1.796  0.07249 .  \nMaxHR            -0.379306   0.142719  -2.658  0.00787 ** \nSexM              0.691529   0.140301   4.929 8.27e-07 ***\n`Age:MaxHR`       0.006517   0.141414   0.046  0.96324    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 724.02  on 522  degrees of freedom\nResidual deviance: 396.98  on 513  degrees of freedom\nAIC: 416.98\n\nNumber of Fisher Scoring iterations: 5\n\nlog_reg_model_2_train_acc &lt;- logistic_M2_fit$results$Accuracy\nlog_reg_model_2_train_acc\n\n[1] 0.8335269\n\nfitted_M2 &lt;- predict(logistic_M2_fit,\n                     newdata = select(test_transformed_2, -HeartDisease_factor), type='raw')\n\nconfusion_matrix_M2 &lt;- confusionMatrix(fitted_M2, test_transformed_2$HeartDisease_factor)\nconfusion_matrix_M2\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 97 28\n         1 20 78\n                                          \n               Accuracy : 0.7848          \n                 95% CI : (0.7249, 0.8368)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 7.614e-16       \n                                          \n                  Kappa : 0.5669          \n                                          \n Mcnemar's Test P-Value : 0.3123          \n                                          \n            Sensitivity : 0.8291          \n            Specificity : 0.7358          \n         Pos Pred Value : 0.7760          \n         Neg Pred Value : 0.7959          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4350          \n   Detection Prevalence : 0.5605          \n      Balanced Accuracy : 0.7825          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#summary and accuracy for logistic M3 model:\nsummary(logistic_M3_fit)\n\n\nCall:\nNULL\n\nCoefficients:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        -0.16713    0.14018  -1.192  0.23316    \nExerciseAnginaY                     0.53876    0.17470   3.084  0.00204 ** \nChestPainTypeATA                   -0.89465    0.19000  -4.709 2.49e-06 ***\nChestPainTypeNAP                   -0.73137    0.17145  -4.266 1.99e-05 ***\nChestPainTypeTA                    -0.19079    0.12338  -1.546  0.12201    \nOldpeak                             0.88793    0.15866   5.596 2.19e-08 ***\nAge                                 0.23408    0.14531   1.611  0.10720    \nMaxHR                              -0.37052    0.14378  -2.577  0.00997 ** \nSexM                                0.69435    0.14006   4.957 7.14e-07 ***\n`Age:MaxHR`                         0.01576    0.14281   0.110  0.91211    \n`ExerciseAnginaY:ChestPainTypeATA`  0.15637    0.12408   1.260  0.20758    \n`ExerciseAnginaY:ChestPainTypeNAP`  0.11526    0.14589   0.790  0.42951    \n`ExerciseAnginaY:ChestPainTypeTA`  -0.02918    0.12090  -0.241  0.80930    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 724.02  on 522  degrees of freedom\nResidual deviance: 394.94  on 510  degrees of freedom\nAIC: 420.94\n\nNumber of Fisher Scoring iterations: 5\n\nlog_reg_model_3_train_acc &lt;- logistic_M3_fit$results$Accuracy\nlog_reg_model_3_train_acc\n\n[1] 0.8258345\n\nfitted_M3 &lt;- predict(logistic_M3_fit,\n                     newdata = test_transformed_2, type='raw')\n\nconfusion_matrix_M3 &lt;- confusionMatrix(fitted_M3, test_transformed_2$HeartDisease_factor)\nconfusion_matrix_M3\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 94 28\n         1 23 78\n                                          \n               Accuracy : 0.7713          \n                 95% CI : (0.7105, 0.8247)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 2.471e-14       \n                                          \n                  Kappa : 0.5405          \n                                          \n Mcnemar's Test P-Value : 0.5754          \n                                          \n            Sensitivity : 0.8034          \n            Specificity : 0.7358          \n         Pos Pred Value : 0.7705          \n         Neg Pred Value : 0.7723          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4215          \n   Detection Prevalence : 0.5471          \n      Balanced Accuracy : 0.7696          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#list of accuracy obtained from each logistic model\nlist(logistic_M1 = confusion_matrix_M1$overall[1], logistic_M2 = confusion_matrix_M2$overall[1], logistic_M3 = confusion_matrix_M3$overall[1])\n\n$logistic_M1\n Accuracy \n0.7847534 \n\n$logistic_M2\n Accuracy \n0.7847534 \n\n$logistic_M3\n Accuracy \n0.7713004 \n\n\nFrom the result after using confusionMatrix(), we can see the the logistic regression Model 1 and Model 2 both have the highest accuracy (78.48%). Considering the complexity of the two models, Model 1 could be chosen as the best model among these three models."
  },
  {
    "objectID": "Homework_5.html#tree-models",
    "href": "Homework_5.html#tree-models",
    "title": "Homework5_ST558_Jia Lu",
    "section": "Tree Models",
    "text": "Tree Models\nTo create tree models, we don’t need to create the dummy variables. Thus, we can use the previous data set “data” in the training process.\nFitting a model with ExerciseAngina, ChestPainType, Oldpeak, Age, MaxHR and Sex as predictors.\n\nClassification tree model\nFitting a model with ExerciseAngina, ChestPainType, Oldpeak, Age, MaxHR and Sex as predictors.\n\nset.seed(11)\n\nclassification_fit &lt;- train(HeartDisease_factor ~ ExerciseAngina + ChestPainType + Oldpeak + Age + MaxHR + Sex,\n                            data = train_transformed_2,\n                            method = \"rpart\",\n                            trControl = trctrl,\n                            tuneGrid = expand.grid(cp = seq(0, 0.1, by = 0.001)))\nclassification_fit\n\nCART \n\n523 samples\n  6 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 471, 470, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.7960571  0.5921520\n  0.001  0.7966981  0.5933808\n  0.002  0.7979681  0.5958697\n  0.003  0.8030842  0.6056964\n  0.004  0.8062288  0.6117823\n  0.005  0.8075472  0.6142758\n  0.006  0.8062893  0.6116555\n  0.007  0.8037494  0.6063203\n  0.008  0.8037494  0.6062862\n  0.009  0.7955128  0.5899115\n  0.010  0.7955128  0.5899115\n  0.011  0.7955128  0.5899115\n  0.012  0.7955128  0.5899882\n  0.013  0.7955128  0.5899882\n  0.014  0.7942066  0.5862887\n  0.015  0.7922835  0.5823505\n  0.016  0.7916425  0.5808631\n  0.017  0.7910015  0.5795391\n  0.018  0.7852201  0.5679580\n  0.019  0.7852201  0.5679580\n  0.020  0.7820150  0.5618399\n  0.021  0.7820150  0.5618399\n  0.022  0.7820150  0.5618399\n  0.023  0.7813861  0.5606915\n  0.024  0.7813861  0.5606915\n  0.025  0.7877358  0.5735517\n  0.026  0.7877358  0.5735517\n  0.027  0.7928399  0.5841780\n  0.028  0.7928399  0.5841780\n  0.029  0.7928399  0.5841780\n  0.030  0.7928399  0.5841780\n  0.031  0.7928399  0.5841780\n  0.032  0.7960208  0.5909013\n  0.033  0.7960208  0.5909013\n  0.034  0.7960208  0.5909013\n  0.035  0.7960208  0.5909013\n  0.036  0.7921746  0.5828725\n  0.037  0.7921746  0.5828725\n  0.038  0.7921746  0.5828725\n  0.039  0.7921746  0.5828725\n  0.040  0.7889695  0.5762485\n  0.041  0.7889695  0.5762485\n  0.042  0.7889695  0.5762485\n  0.043  0.7889695  0.5762485\n  0.044  0.7889695  0.5762485\n  0.045  0.7838413  0.5656251\n  0.046  0.7838413  0.5656251\n  0.047  0.7838413  0.5656251\n  0.048  0.7838413  0.5656251\n  0.049  0.7781084  0.5537281\n  0.050  0.7781084  0.5537281\n  0.051  0.7781084  0.5537281\n  0.052  0.7781084  0.5537281\n  0.053  0.7781084  0.5537281\n  0.054  0.7692066  0.5351377\n  0.055  0.7692066  0.5351377\n  0.056  0.7692066  0.5351377\n  0.057  0.7692066  0.5351377\n  0.058  0.7672835  0.5310580\n  0.059  0.7672835  0.5310580\n  0.060  0.7672835  0.5310580\n  0.061  0.7672835  0.5310580\n  0.062  0.7672835  0.5310580\n  0.063  0.7685535  0.5332146\n  0.064  0.7685535  0.5332146\n  0.065  0.7685535  0.5332146\n  0.066  0.7685535  0.5332146\n  0.067  0.7729923  0.5417999\n  0.068  0.7729923  0.5417999\n  0.069  0.7729923  0.5417999\n  0.070  0.7729923  0.5417999\n  0.071  0.7729923  0.5417999\n  0.072  0.7742743  0.5443299\n  0.073  0.7742743  0.5443299\n  0.074  0.7742743  0.5443299\n  0.075  0.7742743  0.5443299\n  0.076  0.7761974  0.5480965\n  0.077  0.7761974  0.5480965\n  0.078  0.7761974  0.5480965\n  0.079  0.7761974  0.5480965\n  0.080  0.7761974  0.5480965\n  0.081  0.7761974  0.5480965\n  0.082  0.7761974  0.5480965\n  0.083  0.7761974  0.5480965\n  0.084  0.7761974  0.5480965\n  0.085  0.7761974  0.5480965\n  0.086  0.7761974  0.5480965\n  0.087  0.7761974  0.5480965\n  0.088  0.7761974  0.5480965\n  0.089  0.7761974  0.5480965\n  0.090  0.7761974  0.5480965\n  0.091  0.7761974  0.5480965\n  0.092  0.7761974  0.5480965\n  0.093  0.7761974  0.5480965\n  0.094  0.7761974  0.5480965\n  0.095  0.7761974  0.5480965\n  0.096  0.7761974  0.5480965\n  0.097  0.7761974  0.5480965\n  0.098  0.7761974  0.5480965\n  0.099  0.7761974  0.5480965\n  0.100  0.7761974  0.5480965\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.005.\n\n\n\n\nRandom forest tree model\nFitting a model with ExerciseAngina, ChestPainType, Oldpeak, Age, MaxHR and Sex as predictors.\n\nset.seed(11)\n\nrf_fit &lt;- train(HeartDisease_factor ~ ExerciseAngina + ChestPainType + Oldpeak + Age + MaxHR + Sex,\n                data = train_transformed_2,\n                method = \"rf\",\n                trControl = trctrl,\n                tuneGrid = expand.grid(mtry = 1:sqrt(ncol(data)-1)))\nrf_fit\n\nRandom Forest \n\n523 samples\n  6 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 471, 470, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  1     0.8311200  0.6604809\n  2     0.8183358  0.6349310\n  3     0.8037252  0.6059693\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 1.\n\n\n\n\nBoosted tree model\nFitting a model with ExerciseAngina, ChestPainType, Oldpeak, Age, MaxHR and Sex as predictors.\n\nset.seed(11)\n\n#here, \"n.trees\" defines numbers of trees; \"interaction.depth\" defines maximum depth of variable interactions in each tree; \"shrinkage\" defines shrinkage parameter to control overfitting; \"n.minobsinnode\" defines minimum number of observations in each terminal node of a tree; \"verbose\" controls verbosity.\n\nboosted_fit &lt;- train(HeartDisease_factor ~ ExerciseAngina + ChestPainType + Oldpeak + Age + MaxHR + Sex,\n                     data = train_transformed_2,\n                     method = \"gbm\",\n                     trControl = trctrl,\n                     tuneGrid = expand.grid(n.trees = c(25, 50, 100, 200), \n                                            interaction.depth = c(1, 2, 3), \n                                            shrinkage = 0.1, \n                                            n.minobsinnode = 10), \n                     verbose = FALSE ) \nboosted_fit\n\nStochastic Gradient Boosting \n\n523 samples\n  6 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 471, 470, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.8030479  0.6041855\n  1                   50      0.8311321  0.6607735\n  1                  100      0.8335873  0.6659187\n  1                  200      0.8227504  0.6440799\n  2                   25      0.8234881  0.6452593\n  2                   50      0.8317973  0.6621605\n  2                  100      0.8259192  0.6503620\n  2                  200      0.8157475  0.6301770\n  3                   25      0.8266570  0.6517146\n  3                   50      0.8342284  0.6671217\n  3                  100      0.8240566  0.6468044\n  3                  200      0.8069303  0.6123680\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 50, interaction.depth =\n 3, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\n\n\nModels comparison\nObtain basic summary and train accuracy for each model:\n\n#summary and accuracy for classification tree model:\nfitted_classification &lt;- predict(classification_fit,\n                                 newdata = select(test_transformed_2, -HeartDisease_factor),\n                                 type='raw')\n\nconfusion_matrix_classification &lt;- confusionMatrix(fitted_classification, test_transformed_2$HeartDisease_factor)\nconfusion_matrix_classification\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 97 33\n         1 20 73\n                                          \n               Accuracy : 0.7623          \n                 95% CI : (0.7009, 0.8166)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 2.218e-13       \n                                          \n                  Kappa : 0.5207          \n                                          \n Mcnemar's Test P-Value : 0.09929         \n                                          \n            Sensitivity : 0.8291          \n            Specificity : 0.6887          \n         Pos Pred Value : 0.7462          \n         Neg Pred Value : 0.7849          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4350          \n   Detection Prevalence : 0.5830          \n      Balanced Accuracy : 0.7589          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#summary and accuracy for random forest tree model:\nfitted_rf &lt;- predict(rf_fit,\n                     newdata = select(test_transformed_2, -HeartDisease_factor),\n                     type='raw')\n\nconfusion_matrix_rf &lt;- confusionMatrix(fitted_rf, test_transformed_2$HeartDisease_factor)\nconfusion_matrix_rf\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 101  29\n         1  16  77\n                                          \n               Accuracy : 0.7982          \n                 95% CI : (0.7395, 0.8488)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.5931          \n                                          \n Mcnemar's Test P-Value : 0.07364         \n                                          \n            Sensitivity : 0.8632          \n            Specificity : 0.7264          \n         Pos Pred Value : 0.7769          \n         Neg Pred Value : 0.8280          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4529          \n   Detection Prevalence : 0.5830          \n      Balanced Accuracy : 0.7948          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#summary and accuracy for boosted tree model:\nfitted_boosted &lt;- predict(boosted_fit,\n                          newdata = select(test_transformed_2, -HeartDisease_factor),\n                          type='raw')\n\nconfusion_matrix_boosted &lt;- confusionMatrix(fitted_boosted, test_transformed_2$HeartDisease_factor)\nconfusion_matrix_boosted\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 100  26\n         1  17  80\n                                          \n               Accuracy : 0.8072          \n                 95% CI : (0.7492, 0.8568)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.6119          \n                                          \n Mcnemar's Test P-Value : 0.2225          \n                                          \n            Sensitivity : 0.8547          \n            Specificity : 0.7547          \n         Pos Pred Value : 0.7937          \n         Neg Pred Value : 0.8247          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4484          \n   Detection Prevalence : 0.5650          \n      Balanced Accuracy : 0.8047          \n                                          \n       'Positive' Class : 0               \n                                          \n\n##list of accuracy obtained from each tree model\nlist(tree_classification = confusion_matrix_classification$overall[1], tree_rf = confusion_matrix_rf$overall[1], tree_boosted = confusion_matrix_boosted$overall[1])\n\n$tree_classification\n Accuracy \n0.7623318 \n\n$tree_rf\n Accuracy \n0.7982063 \n\n$tree_boosted\n Accuracy \n0.8071749"
  },
  {
    "objectID": "Homework_5.html#wrap-up",
    "href": "Homework_5.html#wrap-up",
    "title": "Homework5_ST558_Jia Lu",
    "section": "Wrap up",
    "text": "Wrap up\nShow accuracy for all the models in a list:\n\nlist(knn = confusion_matrix_knn$overall[1], logistic_M1 = confusion_matrix_M1$overall[1], logistic_M2 = confusion_matrix_M2$overall[1], logistic_M3 = confusion_matrix_M3$overall[1], tree_classification = confusion_matrix_classification$overall[1], tree_rf = confusion_matrix_rf$overall[1], tree_boosted = confusion_matrix_boosted$overall[1])\n\n$knn\n Accuracy \n0.7623318 \n\n$logistic_M1\n Accuracy \n0.7847534 \n\n$logistic_M2\n Accuracy \n0.7847534 \n\n$logistic_M3\n Accuracy \n0.7713004 \n\n$tree_classification\n Accuracy \n0.7623318 \n\n$tree_rf\n Accuracy \n0.7982063 \n\n$tree_boosted\n Accuracy \n0.8071749 \n\n\nFrom the result, we can see that the boosted tree model has the highest accuracy (80.72%) among all the models. Therefore, the boosted tree model overall did the best job (in terms of accuracy) on the test set."
  }
]