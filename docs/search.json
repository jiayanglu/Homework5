[
  {
    "objectID": "Homework_5.html",
    "href": "Homework_5.html",
    "title": "Homework5_ST558_Jia Lu",
    "section": "",
    "text": "What is the purpose of using cross-validation when fitting a random forest model?\n\n\nThe purpose of using cross-validation when fitting a random forest model is to select optimal tuning parameter.\n\n\nDescribe the bagged tree algorithm.\n\n\nBagged tree algorithm uses bagging (or bootstrap aggregation) method to repeatedly get boostrap samples from the data (non-parametric) or a fitted model (parametric) and then calculate bootstrap statistics to obtain standard errors or statistical quantities to construct confidence intervals.\n\n\nWhat is meant by a general linear model?\n\n\nGeneral linear model uses continuous response variables that meet normality assumptions and allows for both continuous and categorical predictors. The relationship between predictors and the response is linear.\n\n\nWhen fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?\n\n\nFitting a multiple linear regression model including an interaction term would accommodate non-additive relationships to represent more complexity among variables which can lead to more accurate predictions compared to not including in the model.\n\n\nWhy do we split our data into a training and test set?\n\n\nSplitting our data into a training and test set is great to avoid overfitting when evaluating the model on the same data it was trained on, and to provide a more robust estimation of model performance. Overall, it ensures the reliability and applicability of the model."
  },
  {
    "objectID": "Homework_5.html#quick-edadata-preparation",
    "href": "Homework_5.html#quick-edadata-preparation",
    "title": "Homework5_ST558_Jia Lu",
    "section": "Quick EDA/Data Preparation",
    "text": "Quick EDA/Data Preparation\n\nQuickly understand data: Check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease.\n\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(corrplot)\n\ncorrplot 0.92 loaded\n\n#read in csv data\ndata &lt;- read_csv('heart.csv')\n\nRows: 918 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Sex, ChestPainType, RestingECG, ExerciseAngina, ST_Slope\ndbl (7): Age, RestingBP, Cholesterol, FastingBS, MaxHR, Oldpeak, HeartDisease\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#check total NA data entries \ncolSums(is.na(data))\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n\nFrom the result, there is no NA entries in this data set.\n\n#check which column has zero values in it and how many of them\ncolSums(data == 0)\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              1            172 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n           704              0              0              0            368 \n      ST_Slope   HeartDisease \n             0            410 \n\n\nFrom the result, we can see there are one zero entry (missing measurement) from RestingBP, and 172 zero entries from Cholesterol. These variables are all numeric variables which might have relationships to HeartDisease. Although FastingBS and Oldpeak also have many zero entries, the value of zero is plausible for these two variables, not due to missing data.\nDrop patients with zero value entries for variable RestingBP and variable Cholesterol from this data set.\n\ndata &lt;- data |&gt;\n  filter(!RestingBP == 0 & !Cholesterol == 0)\ndata\n\n# A tibble: 746 × 12\n     Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1    40 M     ATA                 140         289         0 Normal       172\n 2    49 F     NAP                 160         180         0 Normal       156\n 3    37 M     ATA                 130         283         0 ST            98\n 4    48 F     ASY                 138         214         0 Normal       108\n 5    54 M     NAP                 150         195         0 Normal       122\n 6    39 M     NAP                 120         339         0 Normal       170\n 7    45 F     ATA                 130         237         0 Normal       170\n 8    54 M     ATA                 110         208         0 Normal       142\n 9    37 M     ASY                 140         207         0 Normal       130\n10    48 F     ATA                 120         284         0 Normal       120\n# ℹ 736 more rows\n# ℹ 4 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, ST_Slope &lt;chr&gt;,\n#   HeartDisease &lt;dbl&gt;\n\n\n\nCreate a new variable that is a factor version of the HeartDisease variable (if needed, this depends on how you read in your data). Remove the ST_Slope variable and the original HeartDisease variable (if applicable).\n\n\ndata &lt;- data |&gt;\n  select(everything(), -ST_Slope)\ndata\n\n# A tibble: 746 × 11\n     Age Sex   ChestPainType RestingBP Cholesterol FastingBS RestingECG MaxHR\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1    40 M     ATA                 140         289         0 Normal       172\n 2    49 F     NAP                 160         180         0 Normal       156\n 3    37 M     ATA                 130         283         0 ST            98\n 4    48 F     ASY                 138         214         0 Normal       108\n 5    54 M     NAP                 150         195         0 Normal       122\n 6    39 M     NAP                 120         339         0 Normal       170\n 7    45 F     ATA                 130         237         0 Normal       170\n 8    54 M     ATA                 110         208         0 Normal       142\n 9    37 M     ASY                 140         207         0 Normal       130\n10    48 F     ATA                 120         284         0 Normal       120\n# ℹ 736 more rows\n# ℹ 3 more variables: ExerciseAngina &lt;chr&gt;, Oldpeak &lt;dbl&gt;, HeartDisease &lt;dbl&gt;\n\ndata_2 &lt;- data\ndata_2$HeartDisease &lt;- as.factor(data_2$HeartDisease)\n\n\nCreate dummy columns corresponding to the values of these three variables for use in our kNN fit. The caret vignette has a function to help us out here. You should use dummyVars() and predict() to create new columns. Then add these columns to our data frame.\n\n\n#check categorical variables left in the data set\nstr(data_2)\n\ntibble [746 × 11] (S3: tbl_df/tbl/data.frame)\n $ Age           : num [1:746] 40 49 37 48 54 39 45 54 37 48 ...\n $ Sex           : chr [1:746] \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType : chr [1:746] \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP     : num [1:746] 140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol   : num [1:746] 289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS     : num [1:746] 0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG    : chr [1:746] \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR         : num [1:746] 172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina: chr [1:746] \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak       : num [1:746] 0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDisease  : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ...\n\n\nFrom the output, we have some categorical predictors still in our data set: Sex, ChestPainType, RestingECG, and ExerciseAngina.\n\n#specify all categorical variables left in data\ncategorical_vars &lt;- c(\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\")\n#create new columns for dummy variables via dummyVars() and predict() \ndummy_model &lt;- dummyVars(~ ., data = data_2[, categorical_vars])\ndata_dummies &lt;- predict(dummy_model, newdata = data_2)\n#add these columns to our data frame\ndata_knn &lt;- cbind(data_dummies, data_2[, !names(data_2) %in% categorical_vars])\n\nstr(data_knn)\n\n'data.frame':   746 obs. of  18 variables:\n $ SexF            : num  0 1 0 1 0 0 1 0 0 1 ...\n $ SexM            : num  1 0 1 0 1 1 0 1 1 0 ...\n $ ChestPainTypeASY: num  0 0 0 1 0 0 0 0 1 0 ...\n $ ChestPainTypeATA: num  1 0 1 0 0 0 1 1 0 1 ...\n $ ChestPainTypeNAP: num  0 1 0 0 1 1 0 0 0 0 ...\n $ ChestPainTypeTA : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGLVH   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGNormal: num  1 1 0 1 1 1 1 1 1 1 ...\n $ RestingECGST    : num  0 0 1 0 0 0 0 0 0 0 ...\n $ ExerciseAnginaN : num  1 1 1 0 1 1 1 1 0 1 ...\n $ ExerciseAnginaY : num  0 0 0 1 0 0 0 0 1 0 ...\n $ Age             : num  40 49 37 48 54 39 45 54 37 48 ...\n $ RestingBP       : num  140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol     : num  289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ MaxHR           : num  172 156 98 108 122 170 170 142 130 120 ...\n $ Oldpeak         : num  0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDisease    : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ..."
  },
  {
    "objectID": "Homework_5.html#split-your-data",
    "href": "Homework_5.html#split-your-data",
    "title": "Homework5_ST558_Jia Lu",
    "section": "Split your Data",
    "text": "Split your Data\nSplit this data into a training and test set.\n\nset.seed(11)\n\ntrainIndex &lt;- createDataPartition(data_knn$HeartDisease, p = .7,\n                                  list = FALSE,\n                                  times = 1)\n\ntrain_data &lt;-  data_knn[trainIndex, ]\ntest_data &lt;- data_knn[-trainIndex, ]\n\n#check the dimensions of our training data and testing data frame\ndim(train_data)\n\n[1] 523  18\n\ndim(test_data)\n\n[1] 223  18"
  },
  {
    "objectID": "Homework_5.html#knn",
    "href": "Homework_5.html#knn",
    "title": "Homework5_ST558_Jia Lu",
    "section": "kNN",
    "text": "kNN\nCheck summarized details of our data using summary() method. It will give us a basic idea about our dataset’s attributes range.\n\nsummary(data_knn)\n\n      SexF            SexM       ChestPainTypeASY ChestPainTypeATA\n Min.   :0.000   Min.   :0.000   Min.   :0.000    Min.   :0.0000  \n 1st Qu.:0.000   1st Qu.:1.000   1st Qu.:0.000    1st Qu.:0.0000  \n Median :0.000   Median :1.000   Median :0.000    Median :0.0000  \n Mean   :0.244   Mean   :0.756   Mean   :0.496    Mean   :0.2225  \n 3rd Qu.:0.000   3rd Qu.:1.000   3rd Qu.:1.000    3rd Qu.:0.0000  \n Max.   :1.000   Max.   :1.000   Max.   :1.000    Max.   :1.0000  \n ChestPainTypeNAP ChestPainTypeTA   RestingECGLVH    RestingECGNormal\n Min.   :0.0000   Min.   :0.00000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.0000   1st Qu.:0.00000   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :0.0000   Median :0.00000   Median :0.0000   Median :1.0000  \n Mean   :0.2265   Mean   :0.05496   Mean   :0.2359   Mean   :0.5965  \n 3rd Qu.:0.0000   3rd Qu.:0.00000   3rd Qu.:0.0000   3rd Qu.:1.0000  \n Max.   :1.0000   Max.   :1.00000   Max.   :1.0000   Max.   :1.0000  \n  RestingECGST    ExerciseAnginaN  ExerciseAnginaY       Age       \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :28.00  \n 1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:0.0000   1st Qu.:46.00  \n Median :0.0000   Median :1.0000   Median :0.0000   Median :54.00  \n Mean   :0.1676   Mean   :0.6153   Mean   :0.3847   Mean   :52.88  \n 3rd Qu.:0.0000   3rd Qu.:1.0000   3rd Qu.:1.0000   3rd Qu.:59.00  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :77.00  \n   RestingBP    Cholesterol      FastingBS          MaxHR      \n Min.   : 92   Min.   : 85.0   Min.   :0.0000   Min.   : 69.0  \n 1st Qu.:120   1st Qu.:207.2   1st Qu.:0.0000   1st Qu.:122.0  \n Median :130   Median :237.0   Median :0.0000   Median :140.0  \n Mean   :133   Mean   :244.6   Mean   :0.1676   Mean   :140.2  \n 3rd Qu.:140   3rd Qu.:275.0   3rd Qu.:0.0000   3rd Qu.:160.0  \n Max.   :200   Max.   :603.0   Max.   :1.0000   Max.   :202.0  \n    Oldpeak        HeartDisease\n Min.   :-0.1000   0:390       \n 1st Qu.: 0.0000   1:356       \n Median : 0.5000               \n Mean   : 0.9016               \n 3rd Qu.: 1.5000               \n Max.   : 6.2000               \n\n\nBefore modeling, let’s scale and centralized data.\n\npre_proc_values &lt;- preProcess(train_data, method = c(\"center\", \"scale\"))\n\n#Scaling and centralizing train and test data sets.\ntrain_transformed &lt;- predict(pre_proc_values, train_data)\ntest_transformed &lt;- predict(pre_proc_values, test_data)\n\nFrom above summary statistics, it shows us that all the attributes have a different range. So, we need to standardize our data. We can standardize data using caret’s preProcess() method.\nCaret package provides train() method for training our data for various algorithms. Before train() method, we will first use trainControl() method. It controls the computational nuances of the train() method.\nFor training knn classifier, train() method should be passed with “method” parameter as “knn”. The “trControl” parameter should be passed with results from our trianControl() method. The “tuneLength” parameter holds an integer value. This is for tuning our algorithm.\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\n\nset.seed(11)\nknn_fit &lt;- train(HeartDisease ~ ., data = train_transformed, method = \"knn\",\n                 trControl=trctrl,\n                 tuneGrid = expand.grid(k = 1:40),\n                 tuneLength = 10)\nknn_fit\n\nk-Nearest Neighbors \n\n523 samples\n 17 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 471, 470, ... \nResampling results across tuning parameters:\n\n  k   Accuracy   Kappa    \n   1  0.7540881  0.5065335\n   2  0.7680576  0.5346613\n   3  0.8031447  0.6054836\n   4  0.8031930  0.6058569\n   5  0.8177310  0.6350432\n   6  0.8017658  0.6035295\n   7  0.8158684  0.6315778\n   8  0.8151790  0.6301061\n   9  0.8164731  0.6328184\n  10  0.8138728  0.6276469\n  11  0.8106555  0.6212254\n  12  0.8144533  0.6285959\n  13  0.8183116  0.6362887\n  14  0.8202346  0.6399818\n  15  0.8246009  0.6486054\n  16  0.8272134  0.6537991\n  17  0.8265239  0.6522635\n  18  0.8220247  0.6434136\n  19  0.8175738  0.6345268\n  20  0.8194606  0.6384676\n  21  0.8124456  0.6244117\n  22  0.8194848  0.6383785\n  23  0.8156628  0.6307555\n  24  0.8163159  0.6318452\n  25  0.8201500  0.6395056\n  26  0.8214320  0.6421701\n  27  0.8214441  0.6421971\n  28  0.8252661  0.6498578\n  29  0.8246493  0.6485290\n  30  0.8182753  0.6357533\n  31  0.8215046  0.6422774\n  32  0.8221214  0.6435894\n  33  0.8201984  0.6397962\n  34  0.8215046  0.6423420\n  35  0.8189163  0.6370657\n  36  0.8214925  0.6421690\n  37  0.8189405  0.6370630\n  38  0.8150822  0.6292948\n  39  0.8182874  0.6358193\n  40  0.8138123  0.6269484\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 16.\n\n\nThe output shows Accuracy and Kappa metrics for different k value and it automatically selects best k-value (k=16).\nNow, our model is trained with K value as 16 We are ready to predict HeartDisease for our test set. We can use predict() method.\n\ntest_pred &lt;- predict(knn_fit, newdata = test_transformed)\ntest_pred\n\n  [1] 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0\n [38] 0 1 1 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 0\n [75] 1 0 0 0 1 1 0 0 1 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1\n[112] 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1\n[149] 0 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 0\n[186] 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0\n[223] 0\nLevels: 0 1\n\n\nTo check how well this chosen model does on the test set, we can use the confusionMatrix() function.\n\nconfusion_matrix_knn &lt;- confusionMatrix(test_pred, test_transformed$HeartDisease)\nconfusion_matrix_knn\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 91 27\n         1 26 79\n                                          \n               Accuracy : 0.7623          \n                 95% CI : (0.7009, 0.8166)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 2.218e-13       \n                                          \n                  Kappa : 0.5233          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.7778          \n            Specificity : 0.7453          \n         Pos Pred Value : 0.7712          \n         Neg Pred Value : 0.7524          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4081          \n   Detection Prevalence : 0.5291          \n      Balanced Accuracy : 0.7615          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nIt shows that our model accuracy for test set is 76.23%."
  },
  {
    "objectID": "Homework_5.html#logistic-regression",
    "href": "Homework_5.html#logistic-regression",
    "title": "Homework5_ST558_Jia Lu",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\nLogistic regression Model 1\nMore EDA is conducted on this data set. I create new dummy variables HeartDisease, and calculate and plot correlation between HeartDisease1 and other variables.\n\n#change variable HeartDisease to be a character variable for creating dummy variable purpose\ndata_3 &lt;- data\ndata_3$HeartDisease &lt;- as.character(data_3$HeartDisease)\nstr(data_3)\n\ntibble [746 × 11] (S3: tbl_df/tbl/data.frame)\n $ Age           : num [1:746] 40 49 37 48 54 39 45 54 37 48 ...\n $ Sex           : chr [1:746] \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType : chr [1:746] \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP     : num [1:746] 140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol   : num [1:746] 289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS     : num [1:746] 0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG    : chr [1:746] \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR         : num [1:746] 172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina: chr [1:746] \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak       : num [1:746] 0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDisease  : chr [1:746] \"0\" \"1\" \"0\" \"1\" ...\n\n#create new dummy variables via dummyVars() and predict() \ncategorical_vars_2 &lt;- c(\"Sex\", \"ChestPainType\", \"RestingECG\", \"ExerciseAngina\", \"HeartDisease\")\ndummy_model_2 &lt;- dummyVars(~ ., data = data_3[, categorical_vars_2])\ndata_dummies_2 &lt;- predict(dummy_model_2, newdata = data_3)\n#add these columns to our data frame\ndata_3 &lt;- cbind(data_dummies_2, data_3[, !names(data_3) %in% categorical_vars_2])\nstr(data_3)\n\n'data.frame':   746 obs. of  19 variables:\n $ SexF            : num  0 1 0 1 0 0 1 0 0 1 ...\n $ SexM            : num  1 0 1 0 1 1 0 1 1 0 ...\n $ ChestPainTypeASY: num  0 0 0 1 0 0 0 0 1 0 ...\n $ ChestPainTypeATA: num  1 0 1 0 0 0 1 1 0 1 ...\n $ ChestPainTypeNAP: num  0 1 0 0 1 1 0 0 0 0 ...\n $ ChestPainTypeTA : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGLVH   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECGNormal: num  1 1 0 1 1 1 1 1 1 1 ...\n $ RestingECGST    : num  0 0 1 0 0 0 0 0 0 0 ...\n $ ExerciseAnginaN : num  1 1 1 0 1 1 1 1 0 1 ...\n $ ExerciseAnginaY : num  0 0 0 1 0 0 0 0 1 0 ...\n $ HeartDisease0   : num  1 0 1 0 1 1 1 1 0 1 ...\n $ HeartDisease1   : num  0 1 0 1 0 0 0 0 1 0 ...\n $ Age             : num  40 49 37 48 54 39 45 54 37 48 ...\n $ RestingBP       : num  140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol     : num  289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ MaxHR           : num  172 156 98 108 122 170 170 142 130 120 ...\n $ Oldpeak         : num  0 1 0 1.5 0 0 0 0 1.5 0 ...\n\nm &lt;- cor(select(data_3, -c(\"HeartDisease0\", \"SexF\", \"RestingECGNormal\", \"ExerciseAnginaN\")))\n\ncorrplot(m, method = 'color',\n         order = 'alphabet',\n         diag = FALSE,\n         col = COL2('RdBu'))\n\n\n\n\n\n\n\n\nFrom the graph above it, we can see that there are some positive linear correlation between HeartDisease1 and ExerciseAnginaY, ChestPainTypeASY, Oldpeak, Age, or SexM, and some negative linear correlation between HeartDisease1 and MaxHR, ChestPainTypeATA, or ChestPainTypeNAP. There are also some weak relationships between HeartDisease1 and RestingBP or FastingBS. It seems that Cholesterol, RestingECGLVH, and RestingECGST don’t have strong correlations with HeartDisease1. Additionally, there is also some negative correlation between Age and MaxHR, and positive correlation between ExerciseAnginaY and ChestPainTypeASY.\nTo create logistic regression models, we don’t need to create the dummy variables. Thus, we can use the previous data set “data_2” in the training process.\nFitting a model with ExerciseAngina, ChestPainType, Oldpeak, Age, MaxHR and Sex as predictors.\n\nstr(data_2)\n\ntibble [746 × 11] (S3: tbl_df/tbl/data.frame)\n $ Age           : num [1:746] 40 49 37 48 54 39 45 54 37 48 ...\n $ Sex           : chr [1:746] \"M\" \"F\" \"M\" \"F\" ...\n $ ChestPainType : chr [1:746] \"ATA\" \"NAP\" \"ATA\" \"ASY\" ...\n $ RestingBP     : num [1:746] 140 160 130 138 150 120 130 110 140 120 ...\n $ Cholesterol   : num [1:746] 289 180 283 214 195 339 237 208 207 284 ...\n $ FastingBS     : num [1:746] 0 0 0 0 0 0 0 0 0 0 ...\n $ RestingECG    : chr [1:746] \"Normal\" \"Normal\" \"ST\" \"Normal\" ...\n $ MaxHR         : num [1:746] 172 156 98 108 122 170 170 142 130 120 ...\n $ ExerciseAngina: chr [1:746] \"N\" \"N\" \"N\" \"Y\" ...\n $ Oldpeak       : num [1:746] 0 1 0 1.5 0 0 0 0 1.5 0 ...\n $ HeartDisease  : Factor w/ 2 levels \"0\",\"1\": 1 2 1 2 1 1 1 1 2 1 ...\n\nset.seed(11)\n\n#split this data into a training and test set\ntrainIndex_2 &lt;- createDataPartition(data_2$HeartDisease, p = .7,\n                                  list = FALSE,\n                                  times = 1)\n\ntrain_data_2 &lt;-  data_2[trainIndex_2, ]\ntest_data_2 &lt;- data_2[-trainIndex_2, ]\n#check the dimensions of our training data and testing data frame\ndim(train_data_2)\n\n[1] 523  11\n\ndim(test_data_2)\n\n[1] 223  11\n\n#preprocessing data\npre_proc_values_2 &lt;- preProcess(train_data_2, method = c(\"center\", \"scale\"))\n\n#Scaling and centralizing train and test data sets.\ntrain_transformed_2 &lt;- predict(pre_proc_values_2, train_data_2)\ntest_transformed_2 &lt;- predict(pre_proc_values_2, test_data_2)\n\nlogistic_M1_fit &lt;- train(HeartDisease ~ ExerciseAngina + ChestPainType + Oldpeak + Age + MaxHR + Sex, \n                         data = train_transformed_2, \n                         method = \"glm\",\n                         family=\"binomial\",\n                         trControl=trctrl)\nlogistic_M1_fit\n\nGeneralized Linear Model \n\n523 samples\n  6 predictor\n  2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 470, 470, 471, 471, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8292574  0.6571128\n\n\n\n\nLogistic regression Model 2\nFitting a model with ExerciseAngina, ChestPainType, Oldpeak, Age, MaxHR and Sex, and the interaction term between Age and MaxHR as predictors.\n\nset.seed(11)\nlogistic_M2_fit &lt;- train(HeartDisease ~ ExerciseAngina + ChestPainType + Oldpeak + Age + MaxHR + Sex + Age:MaxHR, \n                         data = train_transformed_2, \n                         method = \"glm\",\n                         family=\"binomial\",\n                         trControl=trctrl,\n                         preProcess = c(\"center\", \"scale\"))\nlogistic_M2_fit\n\nGeneralized Linear Model \n\n523 samples\n  6 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (9), scaled (9) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 471, 470, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8335269  0.6659332\n\n\n\n\nLogistic regression Model 3\n\nset.seed(11)\nlogistic_M3_fit &lt;- train(HeartDisease ~ ExerciseAngina + ChestPainType + Oldpeak + Age + MaxHR + Sex + Age:MaxHR + ExerciseAngina:ChestPainType, \n                         data = train_transformed_2, \n                         method = \"glm\",\n                         family=\"binomial\",\n                         trControl=trctrl,\n                         preProcess = c(\"center\", \"scale\"))\nlogistic_M3_fit\n\nGeneralized Linear Model \n\n523 samples\n  6 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (12), scaled (12) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 471, 471, 471, 471, 471, 470, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8258345  0.6507503\n\n\n\n\nModels Comparison\nProvide basic summary and obtained train accuracy for each model:\n\n#logistic M1 model:\nsummary(logistic_M1_fit)\n\n\nCall:\nNULL\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)       -1.0313     0.3167  -3.257  0.00113 ** \nExerciseAnginaY    1.3451     0.2794   4.814 1.48e-06 ***\nChestPainTypeATA  -1.8349     0.3726  -4.924 8.48e-07 ***\nChestPainTypeNAP  -1.5456     0.3212  -4.812 1.49e-06 ***\nChestPainTypeTA   -0.8061     0.5093  -1.583  0.11350    \nOldpeak            0.8611     0.1563   5.508 3.62e-08 ***\nAge                0.2598     0.1434   1.811  0.07008 .  \nMaxHR             -0.3791     0.1426  -2.658  0.00787 ** \nSexM               1.5854     0.3209   4.940 7.81e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 724.02  on 522  degrees of freedom\nResidual deviance: 396.99  on 514  degrees of freedom\nAIC: 414.99\n\nNumber of Fisher Scoring iterations: 5\n\nlog_reg_model_1_train_acc &lt;- logistic_M1_fit$results$Accuracy\nlog_reg_model_1_train_acc\n\n[1] 0.8292574\n\nfitted_M1 &lt;- predict(logistic_M1_fit,\n                  newdata = test_transformed_2, type='raw')\n\nconfusion_matrix_M1 &lt;- confusionMatrix(fitted_M1, test_transformed_2$HeartDisease)\nconfusion_matrix_M1\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 97 28\n         1 20 78\n                                          \n               Accuracy : 0.7848          \n                 95% CI : (0.7249, 0.8368)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 7.614e-16       \n                                          \n                  Kappa : 0.5669          \n                                          \n Mcnemar's Test P-Value : 0.3123          \n                                          \n            Sensitivity : 0.8291          \n            Specificity : 0.7358          \n         Pos Pred Value : 0.7760          \n         Neg Pred Value : 0.7959          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4350          \n   Detection Prevalence : 0.5605          \n      Balanced Accuracy : 0.7825          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#logistic M2 model:\nsummary(logistic_M2_fit)\n\n\nCall:\nNULL\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -0.115257   0.133244  -0.865  0.38703    \nExerciseAnginaY   0.659364   0.137000   4.813 1.49e-06 ***\nChestPainTypeATA -0.767503   0.155959  -4.921 8.60e-07 ***\nChestPainTypeNAP -0.644370   0.134173  -4.803 1.57e-06 ***\nChestPainTypeTA  -0.181222   0.115074  -1.575  0.11529    \nOldpeak           0.861225   0.156377   5.507 3.64e-08 ***\nAge               0.259045   0.144234   1.796  0.07249 .  \nMaxHR            -0.379306   0.142719  -2.658  0.00787 ** \nSexM              0.691529   0.140301   4.929 8.27e-07 ***\n`Age:MaxHR`       0.006517   0.141414   0.046  0.96324    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 724.02  on 522  degrees of freedom\nResidual deviance: 396.98  on 513  degrees of freedom\nAIC: 416.98\n\nNumber of Fisher Scoring iterations: 5\n\nlog_reg_model_2_train_acc &lt;- logistic_M2_fit$results$Accuracy\nlog_reg_model_2_train_acc\n\n[1] 0.8335269\n\nfitted_M2 &lt;- predict(logistic_M2_fit,\n                  newdata = test_transformed_2, type='raw')\n\nconfusion_matrix_M2 &lt;- confusionMatrix(fitted_M2, test_transformed_2$HeartDisease)\nconfusion_matrix_M2\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 97 28\n         1 20 78\n                                          \n               Accuracy : 0.7848          \n                 95% CI : (0.7249, 0.8368)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 7.614e-16       \n                                          \n                  Kappa : 0.5669          \n                                          \n Mcnemar's Test P-Value : 0.3123          \n                                          \n            Sensitivity : 0.8291          \n            Specificity : 0.7358          \n         Pos Pred Value : 0.7760          \n         Neg Pred Value : 0.7959          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4350          \n   Detection Prevalence : 0.5605          \n      Balanced Accuracy : 0.7825          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#logistic M3 model:\nsummary(logistic_M3_fit)\n\n\nCall:\nNULL\n\nCoefficients:\n                                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                        -0.16713    0.14018  -1.192  0.23316    \nExerciseAnginaY                     0.53876    0.17470   3.084  0.00204 ** \nChestPainTypeATA                   -0.89465    0.19000  -4.709 2.49e-06 ***\nChestPainTypeNAP                   -0.73137    0.17145  -4.266 1.99e-05 ***\nChestPainTypeTA                    -0.19079    0.12338  -1.546  0.12201    \nOldpeak                             0.88793    0.15866   5.596 2.19e-08 ***\nAge                                 0.23408    0.14531   1.611  0.10720    \nMaxHR                              -0.37052    0.14378  -2.577  0.00997 ** \nSexM                                0.69435    0.14006   4.957 7.14e-07 ***\n`Age:MaxHR`                         0.01576    0.14281   0.110  0.91211    \n`ExerciseAnginaY:ChestPainTypeATA`  0.15637    0.12408   1.260  0.20758    \n`ExerciseAnginaY:ChestPainTypeNAP`  0.11526    0.14589   0.790  0.42951    \n`ExerciseAnginaY:ChestPainTypeTA`  -0.02918    0.12090  -0.241  0.80930    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 724.02  on 522  degrees of freedom\nResidual deviance: 394.94  on 510  degrees of freedom\nAIC: 420.94\n\nNumber of Fisher Scoring iterations: 5\n\nlog_reg_model_3_train_acc &lt;- logistic_M3_fit$results$Accuracy\nlog_reg_model_3_train_acc\n\n[1] 0.8258345\n\nfitted_M3 &lt;- predict(logistic_M3_fit,\n                  newdata = test_transformed_2, type='raw')\n\nconfusion_matrix_M3 &lt;- confusionMatrix(fitted_M3, test_transformed_2$HeartDisease)\nconfusion_matrix_M3\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 94 28\n         1 23 78\n                                          \n               Accuracy : 0.7713          \n                 95% CI : (0.7105, 0.8247)\n    No Information Rate : 0.5247          \n    P-Value [Acc &gt; NIR] : 2.471e-14       \n                                          \n                  Kappa : 0.5405          \n                                          \n Mcnemar's Test P-Value : 0.5754          \n                                          \n            Sensitivity : 0.8034          \n            Specificity : 0.7358          \n         Pos Pred Value : 0.7705          \n         Neg Pred Value : 0.7723          \n             Prevalence : 0.5247          \n         Detection Rate : 0.4215          \n   Detection Prevalence : 0.5471          \n      Balanced Accuracy : 0.7696          \n                                          \n       'Positive' Class : 0               \n                                          \n\n#list of accuracy obtained from each logistic model\nlist(logistic_M1 = confusion_matrix_M1$overall[1], logistic_M2 = confusion_matrix_M2$overall[1], logistic_M3 = confusion_matrix_M3$overall[1])\n\n$logistic_M1\n Accuracy \n0.7847534 \n\n$logistic_M2\n Accuracy \n0.7847534 \n\n$logistic_M3\n Accuracy \n0.7713004 \n\n\nFrom the result after using confusionMatrix(), we can see the the logistic regression Model 1 and Model 2 both have the highest accuracy (78.48%). Considering the complexity of the two models, Model 1 could be chosen as the best model among these three models."
  },
  {
    "objectID": "Homework_5.html#tree-models",
    "href": "Homework_5.html#tree-models",
    "title": "Homework5_ST558_Jia Lu",
    "section": "Tree Models",
    "text": "Tree Models"
  },
  {
    "objectID": "Homework_5.html#wrap-up",
    "href": "Homework_5.html#wrap-up",
    "title": "Homework5_ST558_Jia Lu",
    "section": "Wrap up",
    "text": "Wrap up"
  }
]